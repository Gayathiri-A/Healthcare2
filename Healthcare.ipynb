{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536382f8-535f-43de-a6f1-9cbe2cd73115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Gayathiri A\\\\Downloads\\\\archive (1)\\\\heart_attack_prediction_india.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167c4052-e76d-4972-ab11-161eb72d37b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Patient_ID        State_Name  Age  Gender  Diabetes  Hypertension  Obesity  \\\n",
      "0           1         Rajasthan   42  Female         0             0        1   \n",
      "1           2  Himachal Pradesh   26    Male         0             0        0   \n",
      "2           3             Assam   78    Male         0             0        1   \n",
      "3           4            Odisha   58    Male         1             0        1   \n",
      "4           5         Karnataka   22    Male         0             0        0   \n",
      "\n",
      "   Smoking  Alcohol_Consumption  Physical_Activity  ...  Diastolic_BP  \\\n",
      "0        1                    0                  0  ...           119   \n",
      "1        0                    1                  1  ...           115   \n",
      "2        0                    0                  1  ...           117   \n",
      "3        0                    0                  1  ...            65   \n",
      "4        0                    0                  1  ...           109   \n",
      "\n",
      "   Air_Pollution_Exposure  Family_History  Stress_Level  Healthcare_Access  \\\n",
      "0                       1               0             4                  0   \n",
      "1                       0               0             7                  0   \n",
      "2                       0               1            10                  1   \n",
      "3                       0               0             1                  1   \n",
      "4                       0               0             9                  0   \n",
      "\n",
      "   Heart_Attack_History  Emergency_Response_Time  Annual_Income  \\\n",
      "0                     0                      157         611025   \n",
      "1                     0                      331         174527   \n",
      "2                     0                      186        1760112   \n",
      "3                     1                      324        1398213   \n",
      "4                     0                      209          97987   \n",
      "\n",
      "   Health_Insurance  Heart_Attack_Risk  \n",
      "0                 0                  0  \n",
      "1                 0                  0  \n",
      "2                 1                  0  \n",
      "3                 0                  0  \n",
      "4                 0                  1  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 26 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   Patient_ID               10000 non-null  int64 \n",
      " 1   State_Name               10000 non-null  object\n",
      " 2   Age                      10000 non-null  int64 \n",
      " 3   Gender                   10000 non-null  object\n",
      " 4   Diabetes                 10000 non-null  int64 \n",
      " 5   Hypertension             10000 non-null  int64 \n",
      " 6   Obesity                  10000 non-null  int64 \n",
      " 7   Smoking                  10000 non-null  int64 \n",
      " 8   Alcohol_Consumption      10000 non-null  int64 \n",
      " 9   Physical_Activity        10000 non-null  int64 \n",
      " 10  Diet_Score               10000 non-null  int64 \n",
      " 11  Cholesterol_Level        10000 non-null  int64 \n",
      " 12  Triglyceride_Level       10000 non-null  int64 \n",
      " 13  LDL_Level                10000 non-null  int64 \n",
      " 14  HDL_Level                10000 non-null  int64 \n",
      " 15  Systolic_BP              10000 non-null  int64 \n",
      " 16  Diastolic_BP             10000 non-null  int64 \n",
      " 17  Air_Pollution_Exposure   10000 non-null  int64 \n",
      " 18  Family_History           10000 non-null  int64 \n",
      " 19  Stress_Level             10000 non-null  int64 \n",
      " 20  Healthcare_Access        10000 non-null  int64 \n",
      " 21  Heart_Attack_History     10000 non-null  int64 \n",
      " 22  Emergency_Response_Time  10000 non-null  int64 \n",
      " 23  Annual_Income            10000 non-null  int64 \n",
      " 24  Health_Insurance         10000 non-null  int64 \n",
      " 25  Heart_Attack_Risk        10000 non-null  int64 \n",
      "dtypes: int64(24), object(2)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "(10000, 26)\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69548bf-8323-4c60-b728-32dd1d7e87d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset shape: (10000, 24)\n",
      "   State_Name       Age    Gender  Diabetes  Hypertension   Obesity   Smoking  \\\n",
      "0    0.819191 -0.427960 -1.109122 -0.320022     -0.572577  1.514174  1.522449   \n",
      "1   -0.664000 -1.353916  0.901614 -0.320022     -0.572577 -0.660426 -0.656837   \n",
      "2   -1.405595  1.655442  0.901614 -0.320022     -0.572577  1.514174 -0.656837   \n",
      "3    0.571993  0.497997  0.901614  3.124782     -0.572577  1.514174 -0.656837   \n",
      "4   -0.416801 -1.585405  0.901614 -0.320022     -0.572577 -0.660426 -0.656837   \n",
      "\n",
      "   Alcohol_Consumption  Physical_Activity  Diet_Score  ...  Systolic_BP  \\\n",
      "0            -0.738321          -1.214093    1.260457  ...    -1.614293   \n",
      "1             1.354425           0.823660   -0.323708  ...    -0.028084   \n",
      "2            -0.738321           0.823660    0.309958  ...    -1.188725   \n",
      "3            -0.738321           0.823660    1.260457  ...    -1.691669   \n",
      "4            -0.738321           0.823660   -0.006875  ...     0.242733   \n",
      "\n",
      "   Diastolic_BP  Air_Pollution_Exposure  Family_History  Stress_Level  \\\n",
      "0      1.706637                1.215607       -0.672318     -0.529915   \n",
      "1      1.476694               -0.822634       -0.672318      0.516796   \n",
      "2      1.591665               -0.822634        1.487392      1.563507   \n",
      "3     -1.397594               -0.822634       -0.672318     -1.576626   \n",
      "4      1.131779               -0.822634       -0.672318      1.214604   \n",
      "\n",
      "   Healthcare_Access  Heart_Attack_History  Emergency_Response_Time  \\\n",
      "0          -0.671847             -0.424195                -0.439408   \n",
      "1          -0.671847             -0.424195                 1.108826   \n",
      "2           1.488433             -0.424195                -0.181369   \n",
      "3           1.488433              2.357409                 1.046541   \n",
      "4          -0.671847             -0.424195                 0.023282   \n",
      "\n",
      "   Annual_Income  Health_Insurance  \n",
      "0      -0.733249         -0.725271  \n",
      "1      -1.511917         -0.725271  \n",
      "2       1.316607          1.378794  \n",
      "3       0.671015         -0.725271  \n",
      "4      -1.648457         -0.725271  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Patient_ID'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_scaled_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Drop Patient_ID (not useful for modeling)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatient_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Encode categorical variables\u001b[39;00m\n\u001b[0;32m     36\u001b[0m label_encoders \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Patient_ID'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Gayathiri A\\\\Downloads\\\\archive (1)\\\\heart_attack_prediction_india.csv\")\n",
    "\n",
    "# Drop Patient_ID (not useful for modeling)\n",
    "df = df.drop(columns=[\"Patient_ID\"])\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in [\"Gender\", \"State_Name\", \"Health_Insurance\",\"Heart_Attack_Risk\"]:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"Heart_Attack_Risk\"])\n",
    "y = df[\"Heart_Attack_Risk\"]\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame for easier inspection\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"Cleaned dataset shape:\", X_scaled_df.shape)\n",
    "print(X_scaled_df.head())\n",
    "\n",
    "# Drop Patient_ID (not useful for modeling)\n",
    "df = df.drop(columns=[\"Patient_ID\"])\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in [\"Gender\", \"State_Name\", \"Health_Insurance\"]:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"Heart_Attack_Risk\"])\n",
    "y = df[\"Heart_Attack_Risk\"]\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame for easier inspection\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"Cleaned dataset shape:\", X_scaled_df.shape)\n",
    "print(X_scaled_df.head())\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Apply label encoding to categorical columns\n",
    "for col in X.select_dtypes(include=\"object\").columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a49beb0-7b58-45df-bbe1-aed17eecf8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=5000, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e9c77-02a9-4643-b256-93b78ca3c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1bcca2-ad66-47ae-bee2-f7d3724bca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define and fit the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=10000, \n",
    "    solver=\"saga\", \n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "log_reg.fit(X_train, y_train)   # <-- must run before predict\n",
    "\n",
    "# 2. Predictions\n",
    "y_prob_lr = log_reg.predict_proba(X_test)[:, 1]\n",
    "y_pred_lr = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75bacc-0f83-414d-a7f8-79399ec5f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, log_reg.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b51d9-317b-4ab2-9025-4c44fb3ca38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "log_reg_balanced = LogisticRegression(max_iter=5000, class_weight=\"balanced\")\n",
    "log_reg_balanced.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bal = log_reg_balanced.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, log_reg_balanced.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921c773-f211-45c6-9df2-9560ffdc8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Train-test split on balanced data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression again\n",
    "log_reg_smote = LogisticRegression(max_iter=1000)\n",
    "log_reg_smote.fit(X_train, y_train)\n",
    "\n",
    "y_pred_smote = log_reg_smote.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_smote))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, log_reg_smote.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15796ef-c446-431c-85f6-328e533159c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, class_weight=\"balanced\", random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7272f5-c4e5-4d54-876a-b3ac404b5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# scale_pos_weight = (num_negative / num_positive)\n",
    "scale_pos_weight = (y_train.value_counts()[0] / y_train.value_counts()[1])\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=5,\n",
    "                              scale_pos_weight=scale_pos_weight, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, xgb_model.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4a8ea-775b-4c2f-a5b8-e57efc892159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "categorical_cols = [\"State_Name\", \"Gender\", \"Health_Insurance\"]\n",
    "numeric_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_cols),\n",
    "        (\"num\", \"passthrough\", numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(pipeline, \"log_reg_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e172ea-6156-4c6f-8d5c-25b600d1be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Gayathiri A\\\\Downloads\\\\archive (2)\\\\Healthcare_Investments_and_Hospital_Stay (1).csv\")\n",
    "\n",
    "\n",
    "# Target and features\n",
    "y = df[\"Hospital_Stay\"]\n",
    "X = df.drop(columns=[\"Hospital_Stay\"])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Baseline Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "\n",
    "print(\"Linear Regression RÂ²:\", r2_score(y_test, y_pred_lin))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred_lin))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_lin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267fe1b-5636-4667-9b21-af4ab4916623",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3def60-2e7d-4ada-8f2e-f62e40af902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=200, \n",
    "    random_state=42,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "\n",
    "print(\"Random Forest RÂ²:\", r2_score(y_test, y_pred_rf))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred_rf))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180be50-3cef-4d6b-892c-7e691ddac084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_reg.predict(X_test)\n",
    "\n",
    "print(\"XGBoost RÂ²:\", r2_score(y_test, y_pred_xgb))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred_xgb))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_xgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a34aa25c-4431-4666-963d-931abb5f7597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learn\n",
      "Version: 1.6.1\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: https://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: BSD 3-Clause License\n",
      "\n",
      " Copyright (c) 2007-2024 The scikit-learn developers.\n",
      " All rights reserved.\n",
      "\n",
      " Redistribution and use in source and binary forms, with or without\n",
      " modification, are permitted provided that the following conditions are met:\n",
      "\n",
      " * Redistributions of source code must retain the above copyright notice, this\n",
      "   list of conditions and the following disclaimer.\n",
      "\n",
      " * Redistributions in binary form must reproduce the above copyright notice,\n",
      "   this list of conditions and the following disclaimer in the documentation\n",
      "   and/or other materials provided with the distribution.\n",
      "\n",
      " * Neither the name of the copyright holder nor the names of its\n",
      "   contributors may be used to endorse or promote products derived from\n",
      "   this software without specific prior written permission.\n",
      "\n",
      " THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      " AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      " IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      " DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      " FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      " DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      " SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      " CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      " OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      " OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      "Location: C:\\Users\\Gayathiri A\\anaconda3\\Lib\\site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: imbalanced-learn, shap, sklearn-compat\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1489d10-5c2d-48d1-9624-41513908db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Gayathiri A\\\\Downloads\\\\archive (1)\\\\heart_attack_prediction_india.csv\")\n",
    "y = df[\"Heart_Attack_Risk\"]\n",
    "X = df.drop([\"Heart_Attack_Risk\", \"Patient_ID\"], axis=1)\n",
    "\n",
    "categorical_cols = [\"State_Name\", \"Gender\", \"Health_Insurance\"]\n",
    "numeric_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(drop=\"first\"), categorical_cols),\n",
    "    (\"num\", \"passthrough\", numeric_cols)\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(pipeline, \"log_reg_pipeline.pkl\")\n",
    "print(\"âœ… Pipeline retrained and saved with scikit-learn 1.6.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dac0c6-2d4a-4b4e-b386-9dbd43e01a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(drop=\"first\"), categorical_cols),\n",
    "    (\"num\", \"passthrough\", numeric_cols)\n",
    "], remainder=\"drop\")   # drop instead of passthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9125778-2b02-4f08-b6ec-3a391c64afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Gayathiri A\\\\Downloads\\\\archive (1)\\\\heart_attack_prediction_india.csv\")\n",
    "\n",
    "# Target and features\n",
    "y = df[\"Heart_Attack_Risk\"]\n",
    "X = df.drop([\"Heart_Attack_Risk\", \"Patient_ID\"], axis=1)\n",
    "\n",
    "# Define categorical and numeric columns explicitly\n",
    "categorical_cols = [\"State_Name\", \"Gender\", \"Health_Insurance\"]\n",
    "numeric_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "\n",
    "# Preprocessor (no remainder passthrough!)\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(drop=\"first\"), categorical_cols),\n",
    "    (\"num\", StandardScaler(), numeric_cols)\n",
    "])\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "# Train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save pipeline\n",
    "joblib.dump(pipeline, \"log_reg_pipeline.pkl\")\n",
    "print(\"âœ… Pipeline saved successfully with explicit numeric handling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81864c2c-0a3a-4360-8103-1f958dedc576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models/log_reg_pipeline_v1.pkl\")\n",
    "\n",
    "st.title(\"ðŸ¥ Heart Attack Risk Prediction\")\n",
    "\n",
    "# Version guard + safe load\n",
    "def load_pipeline(path: Path):\n",
    "    if not path.exists():\n",
    "        st.error(f\"Model file not found at: {path}\")\n",
    "        st.stop()\n",
    "    try:\n",
    "        obj = joblib.load(path)\n",
    "        saved_version = obj.get(\"sklearn_version\", \"unknown\")\n",
    "        if saved_version != sklearn.__version__:\n",
    "            st.error(f\"Version mismatch: saved {saved_version} vs runtime {sklearn.__version__}. \"\n",
    "                     f\"Retrain in this venv and resave.\")\n",
    "            st.stop()\n",
    "        return obj[\"pipeline\"]\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to load pipeline: {e}\")\n",
    "        st.stop()\n",
    "\n",
    "pipeline = load_pipeline(MODEL_PATH)\n",
    "\n",
    "# Minimal inputs\n",
    "state = st.sidebar.selectbox(\"State_Name\", [\"Manipur\", \"Tamil Nadu\", \"Kerala\"])\n",
    "gender = st.sidebar.selectbox(\"Gender\", [\"Male\", \"Female\"])\n",
    "insurance = st.sidebar.selectbox(\"Health_Insurance\", [\"Yes\", \"No\"])\n",
    "age = st.sidebar.number_input(\"Age\", 0, 120, 45)\n",
    "\n",
    "input_df = pd.DataFrame({\n",
    "    \"State_Name\": [state],\n",
    "    \"Gender\": [gender],\n",
    "    \"Health_Insurance\": [insurance],\n",
    "    \"Age\": [age],\n",
    "    # add other numeric features here exactly as in training\n",
    "})\n",
    "\n",
    "if st.sidebar.button(\"Predict Risk\"):\n",
    "    try:\n",
    "        pred = pipeline.predict(input_df)[0]\n",
    "        proba = pipeline.predict_proba(input_df)[0][1]\n",
    "        st.success(f\"Predicted Risk: {pred} (Confidence: {proba:.2%})\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Prediction failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ff6442-5cb6-453e-ac38-93f0954e3565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gayathiri A\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.6896 - loss: 0.6246 - val_accuracy: 0.7055 - val_loss: 0.6094\n",
      "Epoch 2/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6977 - loss: 0.6096 - val_accuracy: 0.7055 - val_loss: 0.6111\n",
      "Epoch 3/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6984 - loss: 0.6028 - val_accuracy: 0.7045 - val_loss: 0.6138\n",
      "Epoch 4/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6982 - loss: 0.5973 - val_accuracy: 0.7050 - val_loss: 0.6188\n",
      "Epoch 5/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6985 - loss: 0.5907 - val_accuracy: 0.7040 - val_loss: 0.6183\n",
      "Epoch 6/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7005 - loss: 0.5841 - val_accuracy: 0.7040 - val_loss: 0.6241\n",
      "Epoch 7/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7034 - loss: 0.5766 - val_accuracy: 0.7010 - val_loss: 0.6302\n",
      "Epoch 8/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7084 - loss: 0.5690 - val_accuracy: 0.7000 - val_loss: 0.6328\n",
      "Epoch 9/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7130 - loss: 0.5607 - val_accuracy: 0.6945 - val_loss: 0.6432\n",
      "Epoch 10/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7186 - loss: 0.5522 - val_accuracy: 0.6730 - val_loss: 0.6476\n",
      "Epoch 11/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7246 - loss: 0.5431 - val_accuracy: 0.6805 - val_loss: 0.6511\n",
      "Epoch 12/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7321 - loss: 0.5325 - val_accuracy: 0.6825 - val_loss: 0.6592\n",
      "Epoch 13/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7395 - loss: 0.5225 - val_accuracy: 0.6615 - val_loss: 0.6612\n",
      "Epoch 14/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7469 - loss: 0.5130 - val_accuracy: 0.6730 - val_loss: 0.6729\n",
      "Epoch 15/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7526 - loss: 0.5041 - val_accuracy: 0.6235 - val_loss: 0.6789\n",
      "Epoch 16/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7625 - loss: 0.4920 - val_accuracy: 0.6540 - val_loss: 0.6941\n",
      "Epoch 17/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7678 - loss: 0.4817 - val_accuracy: 0.6280 - val_loss: 0.7102\n",
      "Epoch 18/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7750 - loss: 0.4731 - val_accuracy: 0.6465 - val_loss: 0.6976\n",
      "Epoch 19/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7825 - loss: 0.4630 - val_accuracy: 0.6525 - val_loss: 0.7165\n",
      "Epoch 20/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7859 - loss: 0.4566 - val_accuracy: 0.6385 - val_loss: 0.7274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['risk_preprocessor.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Gayathiri A\\Downloads\\archive (1)\\heart_attack_prediction_india.csv\")\n",
    "y = df[\"Heart_Attack_Risk\"]\n",
    "X = df.drop([\"Heart_Attack_Risk\", \"Patient_ID\"], axis=1)\n",
    "\n",
    "categorical_cols = [\"State_Name\", \"Gender\", \"Health_Insurance\"]\n",
    "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), categorical_cols),\n",
    "    (\"num\", StandardScaler(), numeric_cols)\n",
    "])\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build NN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)\n",
    "\n",
    "# Save model\n",
    "model.save(\"risk_dl_model.h5\")\n",
    "joblib.dump(preprocessor, \"risk_preprocessor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1a02b-95a4-402b-9e83-becddaeaaab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af77b4e0-41e4-402b-8a34-8d9e2d5d5974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gayathiri A\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - loss: 56.9995 - mae: 7.1149 - mse: 56.9995 - val_loss: 52.5821 - val_mae: 6.7873 - val_mse: 52.5821\n",
      "Epoch 2/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 50.7977 - mae: 6.6981 - mse: 50.7977 - val_loss: 46.9455 - val_mae: 6.3755 - val_mse: 46.9455\n",
      "Epoch 3/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 44.4312 - mae: 6.2145 - mse: 44.4312 - val_loss: 40.1017 - val_mae: 5.8319 - val_mse: 40.1017\n",
      "Epoch 4/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 36.2034 - mae: 5.5368 - mse: 36.2034 - val_loss: 31.4877 - val_mae: 5.0663 - val_mse: 31.4877\n",
      "Epoch 5/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 26.2154 - mae: 4.5931 - mse: 26.2154 - val_loss: 21.6397 - val_mae: 4.0178 - val_mse: 21.6397\n",
      "Epoch 6/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 16.1725 - mae: 3.3681 - mse: 16.1725 - val_loss: 12.3425 - val_mae: 2.7774 - val_mse: 12.3425\n",
      "Epoch 7/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.3342 - mae: 2.1440 - mse: 8.3342 - val_loss: 6.8396 - val_mae: 1.7843 - val_mse: 6.8396\n",
      "Epoch 8/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 5.1984 - mae: 1.5435 - mse: 5.1984 - val_loss: 5.1359 - val_mae: 1.4421 - val_mse: 5.1359\n",
      "Epoch 9/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 4.5189 - mae: 1.4702 - mse: 4.5189 - val_loss: 4.7007 - val_mae: 1.3411 - val_mse: 4.7007\n",
      "Epoch 10/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 4.0786 - mae: 1.3672 - mse: 4.0786 - val_loss: 4.3908 - val_mae: 1.2769 - val_mse: 4.3908\n",
      "Epoch 11/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.6586 - mae: 1.2570 - mse: 3.6586 - val_loss: 4.1698 - val_mae: 1.2186 - val_mse: 4.1698\n",
      "Epoch 12/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.3553 - mae: 1.1734 - mse: 3.3553 - val_loss: 3.9661 - val_mae: 1.1522 - val_mse: 3.9661\n",
      "Epoch 13/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3.0812 - mae: 1.1025 - mse: 3.0812 - val_loss: 3.7284 - val_mae: 1.0892 - val_mse: 3.7284\n",
      "Epoch 14/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 2.8768 - mae: 1.0502 - mse: 2.8768 - val_loss: 3.5431 - val_mae: 1.0415 - val_mse: 3.5431\n",
      "Epoch 15/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.6618 - mae: 0.9866 - mse: 2.6618 - val_loss: 3.4389 - val_mae: 1.0009 - val_mse: 3.4389\n",
      "Epoch 16/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 2.5118 - mae: 0.9306 - mse: 2.5118 - val_loss: 3.3081 - val_mae: 0.9638 - val_mse: 3.3081\n",
      "Epoch 17/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 2.3707 - mae: 0.8824 - mse: 2.3707 - val_loss: 3.1657 - val_mae: 0.9284 - val_mse: 3.1657\n",
      "Epoch 18/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.2581 - mae: 0.8521 - mse: 2.2581 - val_loss: 3.0736 - val_mae: 0.9016 - val_mse: 3.0736\n",
      "Epoch 19/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 2.1486 - mae: 0.8078 - mse: 2.1486 - val_loss: 2.9766 - val_mae: 0.8777 - val_mse: 2.9766\n",
      "Epoch 20/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 2.0575 - mae: 0.7777 - mse: 2.0575 - val_loss: 2.9020 - val_mae: 0.8599 - val_mse: 2.9020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['los_preprocessor.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load LOS dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Gayathiri A\\Downloads\\archive (2)\\Healthcare_Investments_and_Hospital_Stay (1).csv\")\n",
    "y = df[\"Hospital_Stay\"]\n",
    "X = df.drop([\"Hospital_Stay\"], axis=1)\n",
    "\n",
    "categorical_cols = [\"Location\", \"Time\"]\n",
    "numeric_cols = [\"MRI_Units\", \"CT_Scanners\", \"Hospital_Beds\"]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), categorical_cols),\n",
    "    (\"num\", StandardScaler(), numeric_cols)\n",
    "])\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build NN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)  # regression output\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\", \"mse\"])\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)\n",
    "\n",
    "# Save model\n",
    "model.save(\"los_dl_model.h5\")\n",
    "joblib.dump(preprocessor, \"los_preprocessor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2912cb-197c-48f1-87af-401c564e092c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
